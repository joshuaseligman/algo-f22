%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CMPT 435
% Fall 2022
% Assignment 3
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from: http://www.LaTeXTemplates.com
% Original author: % Frits Wenneker (http://www.howtotex.com)
% License: CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% Modified by Alan G. Labouseur  - alan@labouseur.com
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[letterpaper, 10pt,DIV=13]{scrartcl} 

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm,xfrac} % Math packages
\usepackage{sectsty} % Allows customizing section commands
\usepackage{graphicx}
\usepackage{algorithm, algpseudocode}
\usepackage{listings}
\usepackage{parskip}
\usepackage{lastpage}
\usepackage{color}
\usepackage{qtree}

\allsectionsfont{\normalfont\scshape} % Make all section titles in default font and small caps.

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers

\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{page \thepage\ of \pageref{LastPage}} % Page numbering for right footer

\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs.

\binoppenalty=3000
\relpenalty=3000

\algrenewcommand{\algorithmiccomment}[1]{\hskip1em\textit{$//$ #1}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
   \normalfont \normalsize 
   \textsc{CMPT 435 - Fall 2022 - Dr. Labouseur} \\[10pt] % Header stuff.
   \horrule{0.5pt} \\[0.25cm] 	% Top horizontal rule
   \huge Assignment Three  \\     	    % Assignment title
   \horrule{0.5pt} \\[0.25cm] 	% Bottom horizontal rule
}

\author{Josh Seligman \\ \normalsize joshua.seligman1@marist.edu}

\date{\normalsize\today} 	% Today's date.

\begin{document}
\maketitle % Print the title

\section{Linear Search}
\subsection{The Algorithm}\label{linearSearch}
Linear search is a searching algorithm that walks through an array and continues on until either it finds the target element or reaches the end of the array. As shown in Algorithm \ref{algorithm:linearSearch}, the function has to compare the target value with every element in the array until the condition in the while loop becomes false. Since the entire array is being searched, no assumptions have to be made about the initial status of the array, which means that the array does not have to be sorted or in any particular order prior to running the search.

\begin{algorithm}
  \caption{Linear Search Algorithm}
  \label{algorithm:linearSearch}
  %Documentation for algorithmicx: https://texdoc.org/serve/algorithmicx/0
  \begin{algorithmic}[1]
      \Procedure{LinearSearch}{$arr$, $target$}
        \State $i \gets 0$ \Comment{Start at the beginning of the array}
        \While{$i~<~len(arr)~\&\&~arr[i]~!=~target$} \Comment{Search through the entire array or until the target is found}
          \State $i++$
        \EndWhile
        \If{$i == len(arr)$}
          \State $i = -1$ \Comment{Set $i$ to -1 to note that the target is not in the array}
        \EndIf
        \State return $i$
      \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Asymptotic Analysis and Comparisons}
As mentioned in Section \ref{linearSearch}, performing a linear search requires going through each element until the target is found or until the end of the array is reached. Also, worst case, the number of iterations will be equal to the number of elements in the array. Listing \ref{lst:linearSearch} provides the C++ implementation of linear search and demonstrates the need to iterate through the entire list with its loop on lines 6-12. Therefore, as its name implies, linear search runs in linear time $O(n)$.


\section{Insertion Sort}\label{insertionSortSection}
\subsection{The Algorithm}\label{insertionSortAlgo}
Insertion sort in a sorting algorithm that places an element in its sorted position by sliding previously sorted elements over until the sorted position is found. As shown in Algorithm \ref{algorithm:insertionSort}, the element that is being sorted is only compared to elements in positions $[0, i - 1]$ because these are the elements that have been worked with so far and are known to be in order. Therefore, the elements in this area, as described before, are shifted over until one is less than the value being sorted or until $j < 0$, which will break out of the while loop. Unlike selection sort, the performance of insertion sort varies based on the state of the input array, which will be explored more in detail in Section \ref{insertionAnalysis}.

\begin{algorithm}
  \caption{Insertion Sort Algorithm}
  \label{algorithm:insertionSort}
  %Documentation for algorithmicx: https://texdoc.org/serve/algorithmicx/0
  \begin{algorithmic}[1]
      \Procedure{InsertionSort}{$arr$}
        \For{$i \gets 1,~n - 1$} \Comment{Start at index 1 because the first element is already sorted}
          \State $currentVal \gets arr[i]$
          \State $j \gets i - 1$
          \While{$j \geq 0 ~\textbf{and} ~currentVal < arr[j]$} \Comment{Find the position to place the element}
            \State $arr[j + 1] \gets arr[j]$ \Comment{Shift the element over because it is greater than the current value}
            \State $j \gets j - 1$
          \EndWhile
          \State $arr[j + 1] \gets currentVal$ \Comment{Place the element in its sorted position}
        \EndFor
      \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Asymptotic Analysis and Comparisons}\label{insertionAnalysis}
The code implementation of insertion sort is in Listing \ref{lst:insertionSortListing} on lines 4-33. The worst case for insertion sort is when the list is in reverse order. In this scenario, every element that gets compared to in the inner loop will be greater than the element being sorted, which means that the inner loop will run until $j < 0$ so the item gets placed in the front of the array. As explored in Section \ref{selectionAnalysis}, the total number of iterations in the case of a reversed list for insertion sort will be $\sum_{k = 1} ^{n - 1} k$, which solves to be an $O(n^2)$ runtime. However, the best case for insertion sort is when the array is already sorted. In this situation, the outer loop will still be run, but the inner loop will never be entered because the element being sorted is always going to be greater than or equal to the element in the position before it. Therefore, since the array is only being iterated through from the outer loop, the runtime of insertion sort improves to $\Omega(n)$.

In Table \ref{comparisonsTable}, insertion sort is shown to have 3 very different outcomes for the lists that were used for testing relative to selection sort. First, insertion sort used about half the number of comparisons as selection sort for a list of 666 shuffled magic items. This is because the inner loop of insertion sort may terminate when $arr[j] < currentVal$ (see line 16 in Listing \ref{lst:insertionSortListing}) and in a randomly shuffled list, the probability of $arr[j] < currentVal$ will be around 50\%. Therefore, insertion sort will on average be about 50\% more efficient than selection sort, but is still classified as $O(n^2)$ beause it is still running at a function of $n^2$. Next, when the list is already sorted, insertion sort only makes $n-1$ comparisons. As previously mentioned, the best case for insertion sort is $\Omega(n)$ because the inner loop will never be entered as the second condition for $arr[j] < currentVal$ will always return false. This means there will be only 1 comparison made for each iteration of the outer loop, which equates to $n - 1$ comparisions. Lastly, the worst case for insertion sort is when the list is in reverse order because every element will have to compare itself with all of the elements in the sorted portion of the array. This causes insertion sort to have the same number of comparisons as selection sort for a reversed list at $\frac{1}{2}n^2 - \frac{1}{2}n$, which is also the same number of iterations as the inner loop for insertion sort and makes insertion sort $O(n^2)$.

\section{Merge Sort}
\subsection{The Algorithm}
Merge sort is a divide and conquer sorting algorithm that continues to divide an array up until it has $n$ subarrays of size 1, which, by definition, are all sorted. From there, the subarrays are merged together by comparing the elements in each subarray to determine the sorted order of the combined subarrays. Eventually, the full array will be merged back together will all of the elements fully sorted. As displayed in Algorithm \ref{algorithm:mergeSort} in the $MergeSort$ procedure, since the sort is a divide and conquer algorithm, merge sort takes advantage of recursion to make the problem smaller until it reaches its base case of $length(arr) <= 1$, which is shown on line 2. Additionally, on lines 4 and 5, merge sort always divides a given array in half, which makes its performance very predictable and consistent, which will be discussed more in detail in Section \ref{mergeSortAnalysis}.

\begin{algorithm}
  \caption{Merge Sort Algorithm}
  \label{algorithm:mergeSort}
  %Documentation for algorithmicx: https://texdoc.org/serve/algorithmicx/0
  \begin{algorithmic}[1]
      \Procedure{MergeSort}{$arr$}
        \If{$length(arr) > 1$} \Comment{An array of size 1 is already sorted}
          \State $mid = floor((length(arr)) / 2)$ \Comment{Get the middle index of the array for splitting it in half}
          \State $MergeSort(arr[0:mid])$ \Comment{Perform merge sort on the first half of the array (index 0 - mid, inclusive)}
          \State $MergeSort(arr[mid + 1:length(arr) - 1])$ \Comment{Perform merge sort on the second half of the array}
          \State $Merge(arr, mid)$ \Comment{Merge the 2 subarrays together in order}
        \EndIf
      \EndProcedure
      \\
      \Procedure{Merge}{$arr$, $mid$}
        \State $leftIndex \gets 0$ \Comment{Index for the left subarray}
        \State $rightIndex \gets mid + 1$ \Comment{Index for the right subarray}
        \State $newArr \gets [~]$
        \For{$i \gets 0,~length(arr) - 1$} \Comment{Iterate through all elements}
          \If{$rightIndex >= length(arr)$} \Comment{All the right subarray items are already in $newArr$}
            \State $newArr[i] = arr[leftIndex]$ \Comment{Add the next item from the left subarray}
            \State $leftIndex++$
          \ElsIf{$leftIndex > mid$} \Comment{All the right subarray items are already in $newArr$}
            \State $newArr[i] = arr[rightIndex]$ \Comment{Add the next item from the right subarray}
            \State $rightIndex++$
          \ElsIf{$arr[leftIndex] < arr[rightIndex]$} \Comment{The next element from the left subarray is less than the next element from the right subarray}
            \State $newArr[i] = arr[leftIndex]$ \Comment{Add the next item from the left subarray}
            \State $leftIndex++$
          \Else
            \State $newArr[i] = arr[rightIndex]$ \Comment{Add the next item from the right subarray}
            \State $rightIndex++$
          \EndIf
        \EndFor
        \For{$j \gets 0,~length(arr) - 1$}
          \State $arr[j] \gets newArr[j]$ \Comment{Transfer the sorted elements to the original array}
        \EndFor
      \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Asymptotic Analysis and Comparisons}\label{mergeSortAnalysis}
The C++ implementation of merge sort can be found in Listing \ref{lst:mergeSortListing}, more specifically lines 6-24. As shown on lines 17 and 20, the array is always split in half for each successive call of merge sort until the subarray is of length 1. The recursion tree for merge sort is displayed in Figure \ref{figure:mergeTree}.

\begin{figure}
  \centering
  \caption{Recursion tree for merge sort.}
  \label{figure:mergeTree}
% Documentation: https://www.ling.upenn.edu/advice/latex/qtree/qtreenotes.pdf
  \Tree [.$T(n)$
        [.$T(\frac{n}{2})$
          [.$T(\frac{n}{4})$
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
          ]
          [.$T(\frac{n}{4})$ 
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
          ]
        ]
        [.$T(\frac{n}{2})$
          [.$T(\frac{n}{4})$
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
          ]
          [.$T(\frac{n}{4})$ 
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
          ]
        ]
      ]
\end{figure}

As shown in the recursion tree, the array is broken up into smaller subarrays until the subarray is small enough that it is solved using a constant time operation. Additionally, the last call is on an array of size $\frac{n}{2^k}$. Since the array is being divided in half each time, $k$ is the number of levels in the tree, which is $log_2n$.

Next, after the 2 subarrays are sorted, they are merged together for the conquer phase, which is on lines 26-74 of Listing \ref{lst:mergeSortListing}. The beginning of the function (lines 28-36) are all assignments, which run in constant time. Next, there is a for loop that iterates through the length of the subarray being merged. The body is a large if-else block with comparisons and assignments, which is all $O(1)$ and makes the loop it is contained in run in $O(n)$ time. Lastly, lines 71-73 define a loop that iterates through each element in the subarray, which is also $O(n)$. Therefore, the runtime for the merge function is about $2n + 1$, which is $O(n)$.

At each level of the tree, the merge function will work with exactly $n$ elements. For instance, in the first level of the tree, the left subarray will be merged from 2 subarrays of size $\frac{n}{4}$, which means that the merge function will work with $\frac{n}{2}$ elements. However, since there are 2 subarrays of size $\frac{n}{2}$, the merge function will be called again to handle the other subarray and, therefore, the level will end up merging $n$ elements. Thus, one can multiply $n$ by the number of levels to get the runtime complexity, which is $O(n * log_2n)$.

As shown in Table \ref{comparisonsTable}, merge sort is significantly more efficient at sorting the magic items by using around 5,400 comparisons, which is a little less than the algorithm's runtime of $O(n * log_2n)$. The reason for the number of comparisons being less than 6246.67 ($666 * log_{2}666$) is explained on lines 41 and 45 in Listing \ref{lst:mergeSortListing} as the merging of an entire subarray before the other will result in the algorithm merging the remainder of the other subarray without needing to compare the elements. Regardless, the number of comparisons is still not far from $O(n * log_2n)$. Additionaly, merge sort is very similar to selection sort in that it will perform consistently for any permutation of an array of size $n$. This is shown in the smaller test cases, which perform almost identically despite being in differing orders. Also, one interesting point to note is the performance of insertion sort versus merge sort for the already sorted list. Since insertion sort has a best case of $\Omega(n)$, it is more efficient than merge sort when the array is already mostly sorted. This leads to the idea of hybrid algorithms that may use one sort up until a certain point and switch to a second algorithm that is more efficient in the end game of the sorting process.

\section{Quicksort}
\subsection{The Algorithm}\label{quicksortAlgo}
Quicksort is similar to merge sort in that it is a divide and conquer sorting algorithm. Rather than dividing an array in half and then eventually merging the 2 subarrays back together, quicksort divides an array into 2 partitions around a pivot value so that all elements in the left partition are less than the pivot value and all elements in the right partition are greater than the pivot value. In other words, the elements are getting sorted as they are being divided up, which will result in the entire array being sorted once there are $n$ subarrays of size 1. The performance of quicksort is also dependent on the method used to choose the pivot, which will be discussed in detail in Section \ref{quicksortAnalysis}.

\begin{algorithm}
  \caption{Quicksort Algorithm}
  \label{algorithm:quicksort}
  %Documentation for algorithmicx: https://texdoc.org/serve/algorithmicx/0
  \begin{algorithmic}[1]
      \Procedure{Quicksort}{$arr$}
        \If{$length(arr) > 1$} \Comment{An array of size 1 is already sorted}
          \State $p \gets choosePivotIndex(arr)$ \Comment{More details on how to choose a pivot element in Section \ref{quicksortAnalysis}}
          \State $newPivotIndex \gets Partition(arr, p)$ \Comment{Partition the array around the pivot}
          \State $Quicksort(arr[0:newPivotIndex - 1])$ \Comment{Run quicksort on the left partition}
          \State $Quicksort(arr[newPivotIndex + 1:length(arr) - 1])$ \Comment {Run quicksort on the right partition}
        \EndIf
      \EndProcedure
      \\
      \Procedure{Partition}{$arr$, $pivotIndex$}
        \State $swap(arr, pivotIndex, length(arr) - 1)$ \Comment{Move the pivot to the end of the array}
        \State $lastLowPartitionIndex \gets -1$ \Comment{Initially no elements are in the low partition}
        \For{$i \gets 0,~length(arr) - 2$} \Comment{Iterate through all but the pivot}
          \If{$arr[i] < arr[length(arr) - 1]$}
            \State $lastLowPartitionIndex++$ \Comment{Found another element for the low partition}
            \State $swap(arr, i, lastLowPartitionIndex)$ \Comment{Place the element in the low partition}
          \EndIf
        \EndFor
        \State $swap(arr, length(arr) - 1, lastLowPartitionIndex + 1)$ \Comment{Place the pivot in the appropriate place}
        \State $return lastLowPartitionIndex + 1$
      \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Asyptotic Analysis and Comparisons}\label{quicksortAnalysis}
The C++ implementation for quicksort and partitioning can be found in Listing \ref{lst:quicksortListing}. Since quicksort divides the array into 2 partitions and continues to work until the subarray is of size 1, the recursion tree for quicksort will be the same as merge sort (see Figure \ref{figure:mergeTree}) assuming that the partitions are balanced and approximately the same size. Additionally, the work done to partition the array is $O(n)$ because it iterates through the entire array once (length of subarrays * number of subarrays in the level of the tree). Therefore, on average, the runtime complexity for quicksort will be the same as merge sort at $O(n * log_2n)$.

However, as mentioned in Section \ref{quicksortAlgo}, how the pivot is chosen will impact the runtime in the worst case scenario, which is an already sorted array or a reversed array. In these situations, if the pivot is either the first or last element in the array, the partitions would be of size 0 and size $n - 1$ because the pivot is either going to be greater than all the elements or less than all the elements. As a result, the algorithm would work like selection sort in that it would make sure the one element gets swapped into place and have to iterate through the rest of the array, which is not known to be sorted. Therefore, in these situations, quicksort downgrades to $O(n^2)$.

Therefore, the main problem with quicksort is how to guarantee somewhat balanced partitions. One solution is to use the median of 3 method to pick the partition. The implementation for this approach is found on lines 21-71 of Listing \ref{lst:quicksortListing}. Instead of choosing 1 pivot, the median of 3 approach picks 3 random elements in the array for potential use as the actual pivot and then uses the middle element as the pivot. As a result, there will always be at least 1 element in each partition for all levels of the recursion tree. Even if the partitions are unbalanced, quicksort will still run in linearithmic time, but the logarithm will just have a different base to represent the uneven divide in the elements, and $O(n)$ work will still be done for partitioning at each level of the recursion tree. Also, as shown in the implementation of the median of 3 approach, only random number computations, assignments, and comparisons are needed, which all run in constant time and do not impact the overall runtime complexity of quicksort.

As shown in Table \ref{comparisonsTable}, both the number of comparisons and runtime are most similar to those for merge sort, at approximately $n * log_2n$ comparisons, which lines up with the algorithm's complexity of $O(n * log_2n)$. The number of comparisons for quicksort, however, is greater than that for merge sort for 2 main reasons: quicksort does not have an autocomplete on its partition like merge sort has for merging and there is a tradeoff with the number of comparisons made to prevent $O(n^2)$ runtime. First, since quicksort partitions the array when dividing, every element has to be compared to ensure it is placed in the correct partition. This is very different from merge sort, which just places the rest of a subarray into the merged array when the other subarray being merged is completely merged (lines 41-49 of Listing \ref{lst:mergeSortListing}). Also, as shown in lines 36-71, several comparisons have to be made to pick a pivot that prevents quicksort from downgrading to $O(n^2)$, which is a tradeoff that is made because of how much more efficient $O(n * log_2n)$ is compared to $O(n^2)$. Also, despite having more comparisons than merge sort, quick sort often ran in less time. Although constants are dropped when doing a Big-Oh analysis, the merge function has much larger constants than the partition function for quicksort. As shown in the merge function on lines 26-74 of Listing \ref{lst:mergeSortListing}, there are 2 loops that iterate through the length of the merged subarray, which means that merge sort actually does $2n$ work for each level of the recursion tree. On the other hand, quicksort's partition function on lines 82-114 of Listing \ref{lst:quicksortListing} only iterates through the array once and does $n$ work at each level of the tree, which can save more time as the size of the array increases.

\section{Appendix}
\subsection{Comparisons Table}\label{comparisonsTable}
\begin{center}
  \begin{tabular}{|c|c|c|c|}
    \hline
    Algorithm & List & Comparisons & Time \\
    \hline
    Selection Sort & 666 magic items, shuffled & 221445 & 19916376 ns \\
    \hline
    & 20 Yankees greats, sorted & 190 & 12564 ns \\
    \hline
    & 20 Yankees greats, reversed & 190 & 12104 ns \\
    \hline
    Insertion Sort & 666 magic items, shuffled & 112474 & 8952454 ns \\
    \hline
    & 20 Yankees greats, sorted & 19 & 1586 ns \\
    \hline
    & 20 Yankees greats, reversed & 190 & 13012 ns \\
    \hline
    Merge Sort & 666 magic items, shuffled & 5404 & 1069105 ns \\
    \hline
    & 20 Yankees greats, sorted & 48 & 9518 ns \\
    \hline
    & 20 Yankees greats, reversed & 40 & 6164 ns \\
    \hline
    Quicksort & 666 magic items, shuffled & 8092 & 951497 ns \\
    \hline
    & 20 Yankees greats, sorted & 72 & 8052 ns \\
    \hline
    & 20 Yankees greats, reversed & 75 & 8463 ns \\
    \hline
  \end{tabular}
\captionof{table}{A table of the number of comparisons made and time to complete each sort on a variety of lists.}
\end{center}

\lstset{numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt}

% Colors and lstset for syntax highlighting from https://www.overleaf.com/latex/examples/syntax-highlighting-in-latex-with-the-listings-package/jxnppmxxvsvk
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstset{
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\footnotesize,        % size of fonts used for the code
  breaklines=true,                 % automatic line breaking only at whitespace
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*},          % if you want to add LaTeX within your code
  keywordstyle=\color{blue},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
}

\subsection{Linear Search}
\lstinputlisting[caption = Linear Search (C++), label = lst:linearSearch, language = C++, firstline = 6, lastline = 30, firstnumber = 1]{./../searches.cpp}

\end{document}