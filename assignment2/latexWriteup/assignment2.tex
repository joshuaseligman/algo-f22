%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CMPT 435
% Fall 2022
% Assignment 2
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from: http://www.LaTeXTemplates.com
% Original author: % Frits Wenneker (http://www.howtotex.com)
% License: CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% Modified by Alan G. Labouseur  - alan@labouseur.com
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[letterpaper, 10pt,DIV=13]{scrartcl} 

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm,xfrac} % Math packages
\usepackage{sectsty} % Allows customizing section commands
\usepackage{graphicx}
\usepackage{algorithm, algpseudocode}
\usepackage{listings}
\usepackage{parskip}
\usepackage{lastpage}
\usepackage{color}
\usepackage{qtree}

\allsectionsfont{\normalfont\scshape} % Make all section titles in default font and small caps.

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers

\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{page \thepage\ of \pageref{LastPage}} % Page numbering for right footer

\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs.

\binoppenalty=3000
\relpenalty=3000

\algrenewcommand{\algorithmiccomment}[1]{\hskip1em\textit{$//$ #1}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
   \normalfont \normalsize 
   \textsc{CMPT 435 - Fall 2022 - Dr. Labouseur} \\[10pt] % Header stuff.
   \horrule{0.5pt} \\[0.25cm] 	% Top horizontal rule
   \huge Assignment Two  \\     	    % Assignment title
   \horrule{0.5pt} \\[0.25cm] 	% Bottom horizontal rule
}

\author{Josh Seligman \\ \normalsize joshua.seligman1@marist.edu}

\date{\normalsize\today} 	% Today's date.

\begin{document}
\maketitle % Print the title

\section{Selection Sort}\label{selectionSortSection}
\subsection{The Algorithm}
Selection sort is a sorting algorithm that, for each iteration of the array, selects the smallest (or largest) element of the unsorted part of the array and places the element into its sorted position. As shown in the pseudocode for the sort in Algorithm \ref{algorithm:selectionSort}, selection sort works with the subset of the array in the range $[i,~n)$ in each iteration because the elements in the indices less than $i$ are already sorted and do not have to be checked. Thus, as more elements get sorted, the quicker each iteration becomes because a smaller portion of the array is compared until $i = n - 2$, which is the final iteration of the algorithm. Selection sort is also very consistent in that it runs in the same amount of time regardless of the order of the elements and has both a best and worst case of $O(n^2)$, which will be analyzed in further detail in Section \ref{selectionAnalysis}.

\begin{algorithm}
  \caption{Selection Sort Algorithm}
  \label{algorithm:selectionSort}
  %Documentation for algorithmicx: https://texdoc.org/serve/algorithmicx/0
  \begin{algorithmic}[1]
      \Procedure{SelectionSort}{$arr$}
        \For{$i \gets 0,~n - 2$} \Comment{Iterate through the second to last element as an array of size 1 is sorted}
          \State $smallestIndex \gets i$
          \For{$j \gets i + 1,~n - 1$} \Comment{Iterate through the remainder of the array}
            \If{$arr[j] < arr[smallestIndex]$}
              \State $smallestIndex \gets j$ \Comment{Set the new smallest index if a smaller element is found}
            \EndIf
          \EndFor
          \State $swap(arr,~i,~smallestIndex)$ \Comment{Place the smallest item in the subarray into its sorted place}
        \EndFor
      \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Asymptotic Analysis and Comparisons}\label{selectionAnalysis}
Listing \ref{lst:selectionSortListing} contains the C++ code implementing selection sort on lines 3-24. Line 3 defines a loop that iterates $n - 1$ times and contains 2 assignments and a comparison, all of which operate in constant time for each iteration. Thus, line 3 will take $(n - 1) * C_{1}$ time, where $C_{1}$ is the time needed for each of the operations. Next, line 5 is an assignment, which runs in constant time and executes $n - 1$ times because it is in the outer loop, resulting in a time of $(n - 1) * C_{2}$, where $C_{2}$ is the time needed for the assignment. Line 8, similar to line 3, defines a loop with 3 constant time expressions, which can be marked as $C_{3}$. However, since it is nested inside of the loop on line 3, the total number of iterations of the inner loop is more complex. In the first iteration of the outer loop, the inner loop runs $n - 1$ times. From there, each corresponding iteration of the outer loop results in one less iteration of the inner loop with a minimum of 1 pass on the inner loop when $i = n - 2$. Therefore, the total number of times the inner loop on line 8 will be called is $\sum_{k = 1} ^{n - 1} k$, which by the formula for the sum of the first $N$ natural numbers, is equal to $\frac{(n - 1)(n - 1 + 1)}{2} = \frac{1}{2}n^2 - \frac{1}{2}n$. Thus, the total time to execute line 8 is $(\frac{1}{2}n^2 - \frac{1}{2}n) * C_{3}$. Next, line 10 contains a comparison that, since it is nested inside the inner loop, will run in $(\frac{1}{2}n^2 - \frac{1}{2}n) * C_{4}$ time, where $C_{4}$ is the time needed to make the comparison. Line 12 is a simple assignment and, just like line 10, will run in $(\frac{1}{2}n^2 - \frac{1}{2}n) * C_{5}$, where $C_{5}$ is the time to perform the assignment. The comparison and assignment on lines 15 and 16 are purely for collecting data and not part of the algorithm and, therefore, will be excluded from the asymptotic analysis of selection sort. Line 18 is the end of the inner loop, and represents an unconditional branch back to the top of the loop, which means it runs the same number of iterations as the loop, which is $(\frac{1}{2}n^2 - \frac{1}{2}n) * C_{6}$, where $C_{6}$ is the time needed to execute the branch. Next, lines 22-24 are all assigments, which run in constant time, and are located in the outer loop. Thus, they run in $(n - 1) * C_{7}$ time, where $C_{7}$ is the time needed to perform the swap. Lastly, line 24 is the close and unconditional branch for the outer loop, which will run in $(n - 1) * C_{8}$ time, where $C_{8}$ is the time to execute the unconditional branch. Overall, when adding up the runtimes of each line and dropping the constants, the sum is $4 * (n - 1) + 4 * (\frac{1}{2}n^2 - \frac{1}{2}n) = 2n^2 + 2n - 4 \approx n^2 + n$ is $O(n^2)$.

As shown in Table \ref{comparisonsTable}, selection sort is very consistent with the number of comparisons made as, regardless of the state of the list, it always makes $\frac{1}{2}n^2 - \frac{1}{2}n$ comparisons. This is no coincidence as it is also the number of times the algorithm's inner loop iterates, which means that the selection sort will run very consistently for all lists, no matter the state of the array prior to running the algorithm.

\section{Insertion Sort}\label{insertionSortSection}
\subsection{The Algorithm}\label{insertionSortAlgo}
Insertion sort in a sorting algorithm that places an element in its sorted position by sliding previously sorted elements over until the sorted position is found. As shown in Algorithm \ref{algorithm:insertionSort}, the element that is being sorted is only compared to elements in positions $[0, i - 1]$ because these are the elements that have been worked with so far and are known to be in order. Therefore, the elements in this area, as described before, are shifted over until one is less than the value being sorted or until $j < 0$, which will break out of the while loop. Unlike selection sort, the performance of insertion sort varies based on the state of the input array, which will be explored more in detail in Section \ref{insertionAnalysis}.

\begin{algorithm}
  \caption{Insertion Sort Algorithm}
  \label{algorithm:insertionSort}
  %Documentation for algorithmicx: https://texdoc.org/serve/algorithmicx/0
  \begin{algorithmic}[1]
      \Procedure{InsertionSort}{$arr$}
        \For{$i \gets 1,~n - 1$} \Comment{Start at index 1 because the first element is already sorted}
          \State $currentVal \gets arr[i]$
          \State $j \gets i - 1$
          \While{$j \geq 0 ~\textbf{and} ~currentVal < arr[j]$} \Comment{Find the position to place the element}
            \State $arr[j + 1] \gets arr[j]$ \Comment{Shift the element over because it is greater than the current value}
            \State $j \gets j - 1$
          \EndWhile
          \State $arr[j + 1] \gets currentVal$ \Comment{Place the element in its sorted position}
        \EndFor
      \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Asymptotic Analysis and Comparisons}\label{insertionAnalysis}
The code implementation of insertion sort is in Listing \ref{lst:insertionSortListing} on lines 4-33. The worst case for insertion sort is when the list is in reverse order. In this scenario, every element that gets compared to in the inner loop will be greater than the element being sorted, which means that the inner loop will run until $j < 0$ so the item gets placed in the front of the array. As explored in Section \ref{selectionAnalysis}, the total number of iterations in the case of a reversed list for insertion sort will be $\sum_{k = 1} ^{n - 1} k$, which solves to be an $O(n^2)$ runtime. However, the best case for insertion sort is when the array is already sorted. In this situation, the outer loop will still be run, but the inner loop will never be entered because the element being sorted is always going to be greater than or equal to the element in the position before it. Therefore, since the array is only being iterated through from the outer loop, the runtime of insertion sort improves to $\Omega(n)$.

In Table \ref{comparisonsTable}, insertion sort is shown to have 3 very different outcomes for the lists that were used for testing relative to selection sort. First, insertion sort used about half the number of comparisons as selection sort for a list of 666 shuffled magic items. This is because the inner loop of insertion sort may terminate when $arr[j] < currentVal$ (see line 16 in Listing \ref{lst:insertionSortListing}) and in a randomly shuffled list, the probability of $arr[j] < currentVal$ will be around 50\%. Therefore, insertion sort will on average be about 50\% more efficient than selection sort, but is still classified as $O(n^2)$ beause it is still running at a function of $n^2$. Next, when the list is already sorted, insertion sort only makes $n-1$ comparisons. As previously mentioned, the best case for insertion sort is $\Omega(n)$ because the inner loop will never be entered as the second condition for $arr[j] < currentVal$ will always return false. This means there will be only 1 comparison made for each iteration of the outer loop, which equates to $n - 1$ comparisions. Lastly, the worst case for insertion sort is when the list is in reverse order because every element will have to compare itself with all of the elements in the sorted portion of the array. This causes insertion sort to have the same number of comparisons as selection sort for a reversed list at $\frac{1}{2}n^2 - \frac{1}{2}n$, which is also the same number of iterations as the inner loop for insertion sort and makes insertion sort $O(n^2)$.

\section{Merge Sort}
\subsection{The Algorithm}
Merge sort is a divide and conquer sorting algorithm that continues to divide an array up until it has $n$ subarrays of size 1, which, by definition, are all sorted. From there, the subarrays are merged together by comparing the elements in each subarray to determine the sorted order of the combined subarrays. Eventually, the full array will be merged back together will all of the elements fully sorted. As displayed in Algorithm \ref{algorithm:mergeSort} in the $MergeSort$ procedure, since the sort is a divide and conquer algorithm, merge sort takes advantage of recursion to make the problem smaller until it reaches its base case of $length(arr) <= 1$, which is shown on line 2. Additionally, on lines 4 and 5, merge sort always divides a given array in half, which makes its performance very predictable and consistent, which will be discussed more in detail in Section \ref{mergeSortAnalysis}.

\begin{algorithm}
  \caption{Merge Sort Algorithm}
  \label{algorithm:mergeSort}
  %Documentation for algorithmicx: https://texdoc.org/serve/algorithmicx/0
  \begin{algorithmic}[1]
      \Procedure{MergeSort}{$arr$}
        \If{$length(arr) > 1$} \Comment{An array of size 1 is already sorted}
          \State $mid = floor((length(arr)) / 2)$ \Comment{Get the middle index of the array for splitting it in half}
          \State $MergeSort(arr[0:mid])$ \Comment{Perform merge sort on the first half of the array (index 0 - mid, inclusive)}
          \State $MergeSort(arr[mid + 1:length(arr) - 1])$ \Comment{Perform merge sort on the second half of the array}
          \State $Merge(arr, mid)$ \Comment{Merge the 2 subarrays together in order}
        \EndIf
      \EndProcedure
      \\
      \Procedure{Merge}{$arr$, $mid$}
        \State $leftIndex \gets 0$ \Comment{Index for the left subarray}
        \State $rightIndex \gets mid + 1$ \Comment{Index for the right subarray}
        \State $newArr \gets [~]$
        \For{$i \gets 0,~length(arr) - 1$} \Comment{Iterate through all elements}
          \If{$rightIndex >= length(arr)$} \Comment{All the right subarray items are already in $newArr$}
            \State $newArr[i] = arr[leftIndex]$ \Comment{Add the next item from the left subarray}
            \State $leftIndex++$
          \ElsIf{$leftIndex > mid$} \Comment{All the right subarray items are already in $newArr$}
            \State $newArr[i] = arr[rightIndex]$ \Comment{Add the next item from the right subarray}
            \State $rightIndex++$
          \ElsIf{$arr[leftIndex] < arr[rightIndex]$} \Comment{The next element from the left subarray is less than the next element from the right subarray}
            \State $newArr[i] = arr[leftIndex]$ \Comment{Add the next item from the left subarray}
            \State $leftIndex++$
          \Else
            \State $newArr[i] = arr[rightIndex]$ \Comment{Add the next item from the right subarray}
            \State $rightIndex++$
          \EndIf
        \EndFor
        \For{$j \gets 0,~length(arr) - 1$}
          \State $arr[j] \gets newArr[j]$ \Comment{Transfer the sorted elements to the original array}
        \EndFor
      \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Asymptotic Analysis and Comparisons}\label{mergeSortAnalysis}
The C++ implementation of merge sort can be found in Listing \ref{lst:mergeSortListing}, more specifically lines 6-24. As shown on lines 17 and 20, the array is always split in half for each successive call of merge sort until the subarray is of length 1. The recursion tree for merge sort is displayed in Figure \ref{figure:mergeTree}.

\begin{figure}
  \centering
  \caption{Recursion tree for merge sort.}
  \label{figure:mergeTree}
% Documentation: https://www.ling.upenn.edu/advice/latex/qtree/qtreenotes.pdf
  \Tree [.$T(n)$
        [.$T(\frac{n}{2})$
          [.$T(\frac{n}{4})$
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
          ]
          [.$T(\frac{n}{4})$ 
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
          ]
        ]
        [.$T(\frac{n}{2})$
          [.$T(\frac{n}{4})$
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
          ]
          [.$T(\frac{n}{4})$ 
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
            [.{...} 
              [.$T(\frac{n}{2^k})$ $O(1)$ ]
            ]
          ]
        ]
      ]
\end{figure}

As shown in the recursion tree, the array is broken up into smaller subarrays until the subarray is small enough that it is solved using a constant time operation. Additionally, the last call is on an array of size $\frac{n}{2^k}$. Since the array is being divided in half each time, $k$ is the number of levels in the tree, which is $log_2n$.

Next, after the 2 subarrays are sorted, they are merged together for the conquer phase, which is on lines 26-74 of Listing \ref{lst:mergeSortListing}. The beginning of the function (lines 28-36) are all assignments, which run in constant time. Next, there is a for loop that iterates through the length of the subarray being merged. The body is a large if-else block with comparisons and assignments, which is all $O(1)$ and makes the loop it is contained in run in $O(n)$ time. Lastly, lines 71-73 define a loop that iterates through each element in the subarray, which is also $O(n)$. Therefore, the runtime for the merge function is about $2n + 1$, which is $O(n)$.

At each level of the tree, the merge function will work with exactly $n$ elements. For instance, in the first level of the tree, the left subarray will be merged from 2 subarrays of size $\frac{n}{4}$, which means that the merge function will work with $\frac{n}{2}$ elements. However, since there are 2 subarrays of size $\frac{n}{2}$, the merge function will be called again to handle the other subarray and, therefore, the level will end up merging $n$ elements. Thus, one can multiply $n$ by the number of levels to get the runtime complexity, which is $O(n * log_2n)$.

As shown in Table \ref{comparisonsTable}, merge sort is significantly more efficient at sorting the magic items by using around 5,400 comparisons, which is a little less than the algorithm's runtime of $O(n * log_2n)$. The reason for the number of comparisons being less than 6246.67 ($666 * log_{2}666$) is explained on lines 41 and 45 in Listing \ref{lst:mergeSortListing} as the merging of an entire subarray before the other will result in the algorithm merging the remainder of the other subarray without needing to compare the elements. Regardless, the number of comparisons is still not far from $O(n * log_2n)$. Additionaly, merge sort is very similar to selection sort in that it will perform consistently for any permutation of an array of size $n$. This is shown in the smaller test cases, which perform almost identically despite being in differing orders. Also, one interesting point to note is the performance of insertion sort versus merge sort for the already sorted list. Since insertion sort has a best case of $\Omega(n)$, it is more efficient than merge sort when the array is already mostly sorted. This leads to the idea of hybrid algorithms that may use one sort up until a certain point and switch to a second algorithm that is more efficient in the end game of the sorting process.

\section{Quicksort}
\subsection{The Algorithm}\label{quicksortAlgo}
Quicksort is similar to merge sort in that it is a divide and conquer sorting algorithm. Rather than dividing an array in half and then eventually merging the 2 subarrays back together, quicksort divides an array into 2 partitions around a pivot value so that all elements in the left partition are less than the pivot value and all elements in the right partition are greater than the pivot value. In other words, the elements are getting sorted as they are being divided up, which will result in the entire array being sorted once there are $n$ subarrays of size 1. The performance of quicksort is also dependent on the method used to choose the pivot, which will be discussed in detail in Section \ref{quicksortAnalysis}.

\begin{algorithm}
  \caption{Quicksort Algorithm}
  \label{algorithm:quicksort}
  %Documentation for algorithmicx: https://texdoc.org/serve/algorithmicx/0
  \begin{algorithmic}[1]
      \Procedure{Quicksort}{$arr$}
        \If{$length(arr) > 1$} \Comment{An array of size 1 is already sorted}
          \State $p \gets choosePivotIndex(arr)$ \Comment{More details on how to choose a pivot element in Section \ref{quicksortAnalysis}}
          \State $newPivotIndex \gets Partition(arr, p)$ \Comment{Partition the array around the pivot}
          \State $Quicksort(arr[0:newPivotIndex - 1])$ \Comment{Run quicksort on the left partition}
          \State $Quicksort(arr[newPivotIndex + 1:length(arr) - 1])$ \Comment {Run quicksort on the right partition}
        \EndIf
      \EndProcedure
      \\
      \Procedure{Partition}{$arr$, $pivotIndex$}
        \State $swap(arr, pivotIndex, length(arr) - 1)$ \Comment{Move the pivot to the end of the array}
        \State $lastLowPartitionIndex \gets -1$ \Comment{Initially no elements are in the low partition}
        \For{$i \gets 0,~length(arr) - 2$} \Comment{Iterate through all but the pivot}
          \If{$arr[i] < arr[length(arr) - 1]$}
            \State $lastLowPartitionIndex++$ \Comment{Found another element for the low partition}
            \State $swap(arr, i, lastLowPartitionIndex)$ \Comment{Place the element in the low partition}
          \EndIf
        \EndFor
        \State $swap(arr, length(arr) - 1, lastLowPartitionIndex + 1)$ \Comment{Place the pivot in the appropriate place}
        \State $return lastLowPartitionIndex + 1$
      \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Asyptotic Analysis and Comparisons}\label{quicksortAnalysis}
The C++ implementation for quicksort and partitioning can be found in Listing \ref{lst:quicksortListing}. Since quicksort divides the array into 2 partitions and continues to work until the subarray is of size 1, the recursion tree for quicksort will be the same as merge sort (see Figure \ref{figure:mergeTree}) assuming that the partitions are balanced and approximately the same size. Additionally, the work done to partition the array is $O(n)$ because it iterates through the entire array once (length of subarrays * number of subarrays in the level of the tree). Therefore, on average, the runtime complexity for quicksort will be the same as merge sort at $O(n * log_2n)$.

However, as mentioned in Section \ref{quicksortAlgo}, how the pivot is chosen will impact the runtime in the worst case scenario, which is an already sorted array or a reversed array. In these situations, if the pivot is either the first or last element in the array, the partitions would be of size 0 and size $n - 1$ because the pivot is either going to be greater than all the elements or less than all the elements. As a result, the algorithm would work like selection sort in that it would make sure the one element gets swapped into place and have to iterate through the rest of the array, which is not known to be sorted. Therefore, in these situations, quicksort downgrades to $O(n^2)$.

Therefore, the main problem with quicksort is how to guarantee somewhat balanced partitions. One solution is to use the median of 3 method to pick the partition. The implementation for this approach is found on lines 21-71 of Listing \ref{lst:quicksortListing}. Instead of choosing 1 pivot, the median of 3 approach picks 3 random elements in the array for potential use as the actual pivot and then uses the middle element as the pivot. As a result, there will always be at least 1 element in each partition for all levels of the recursion tree. Even if the partitions are unbalanced, quicksort will still run in linearithmic time, but the logarithm will just have a different base to represent the uneven divide in the elements, and $O(n)$ work will still be done for partitioning at each level of the recursion tree. Also, as shown in the implementation of the median of 3 approach, only random number computations, assignments, and comparisons are needed, which all run in constant time and do not impact the overall runtime complexity of quicksort.

As shown in Table \ref{comparisonsTable}, both the number of comparisons and runtime are most similar to those for merge sort, at approximately $n * log_2n$ comparisons, which lines up with the algorithm's complexity of $O(n * log_2n)$. The number of comparisons for quicksort, however, is greater than that for merge sort for 2 main reasons: quicksort does not have an autocomplete on its partition like merge sort has for merging and there is a tradeoff with the number of comparisons made to prevent $O(n^2)$ runtime. First, since quicksort partitions the array when dividing, every element has to be compared to ensure it is placed in the correct partition. This is very different from merge sort, which just places the rest of a subarray into the merged array when the other subarray being merged is completely merged (lines 41-49 of Listing \ref{lst:mergeSortListing}). Also, as shown in lines 36-71, several comparisons have to be made to pick a pivot that prevents quicksort from downgrading to $O(n^2)$, which is a tradeoff that is made because of how much more efficient $O(n * log_2n)$ is compared to $O(n^2)$. Also, despite having more comparisons than merge sort, quick sort often ran in less time. Although constants are dropped when doing a Big-Oh analysis, the merge function has much larger constants than the partition function for quicksort. As shown in the merge function on lines 26-74 of Listing \ref{lst:mergeSortListing}, there are 2 loops that iterate through the length of the merged subarray, which means that merge sort actually does $2n$ work for each level of the recursion tree. On the other hand, quicksort's partition function on lines 82-114 of Listing \ref{lst:quicksortListing} only iterates through the array once and does $n$ work at each level of the tree, which can save more time as the size of the array increases.

\section{Appendix}
\subsection{Comparisons Table}\label{comparisonsTable}
\begin{center}
  \begin{tabular}{|c|c|c|c|}
    \hline
    Algorithm & List & Comparisons & Time \\
    \hline
    Selection Sort & 666 magic items, shuffled & 221445 & 19916376 ns \\
    \hline
    & 20 Yankees greats, sorted & 190 & 12564 ns \\
    \hline
    & 20 Yankees greats, reversed & 190 & 12104 ns \\
    \hline
    Insertion Sort & 666 magic items, shuffled & 112474 & 8952454 ns \\
    \hline
    & 20 Yankees greats, sorted & 19 & 1586 ns \\
    \hline
    & 20 Yankees greats, reversed & 190 & 13012 ns \\
    \hline
    Merge Sort & 666 magic items, shuffled & 5404 & 1069105 ns \\
    \hline
    & 20 Yankees greats, sorted & 48 & 9518 ns \\
    \hline
    & 20 Yankees greats, reversed & 40 & 6164 ns \\
    \hline
    Quicksort & 666 magic items, shuffled & 8092 & 951497 ns \\
    \hline
    & 20 Yankees greats, sorted & 72 & 8052 ns \\
    \hline
    & 20 Yankees greats, reversed & 75 & 8463 ns \\
    \hline
  \end{tabular}
\captionof{table}{A table of the number of comparisons made and time to complete each sort on a variety of lists.}
\end{center}

\lstset{numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt}

% Colors and lstset for syntax highlighting from https://www.overleaf.com/latex/examples/syntax-highlighting-in-latex-with-the-listings-package/jxnppmxxvsvk
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstset{
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\footnotesize,        % size of fonts used for the code
  breaklines=true,                 % automatic line breaking only at whitespace
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*},          % if you want to add LaTeX within your code
  keywordstyle=\color{blue},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
}

\subsection{Selection Sort}\label{selectionSortListing}
\lstinputlisting[caption = Selection Sort (C++), label = lst:selectionSortListing, language = C++, firstline = 8, lastline = 32, firstnumber = 1]{./../sortsAndShuffles.cpp}

\subsection{Insertion Sort}\label{insertionSortListing}
\lstinputlisting[caption = Insertion Sort (C++), label = lst:insertionSortListing, language = C++, firstline = 34, lastline = 67, firstnumber = 1]{./../sortsAndShuffles.cpp}

\subsection{Merge Sort}\label{mergeSortListing}
\lstinputlisting[caption = Merge Sort (C++), label = lst:mergeSortListing, language = C++, firstline = 69, lastline = 142, firstnumber = 1]{./../sortsAndShuffles.cpp}

\subsection{Quicksort}\label{quicksortListing}
\lstinputlisting[caption = Quicksort (C++), label = lst:quicksortListing, language = C++, firstline = 144, lastline = 257, firstnumber = 1]{./../sortsAndShuffles.cpp}

\end{document}